{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def vitrine_vagas_gupy(LINK, nome_vaga):\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(LINK)\n",
    "    driver.maximize_window()\n",
    "    try:\n",
    "        cuidado_golpes = driver.find_element(By.XPATH, '//*[@id=\"radix-0\"]/div[2]/button')\n",
    "        cuidado_golpes.click()\n",
    "    except:\n",
    "        pass\n",
    "    search = driver.find_element(By.XPATH, '/html/body/div/main/div/div/div[2]/div/div/div/div/div[2]/input')\n",
    "    search.send_keys(nome_vaga)\n",
    "    search.send_keys(Keys.ENTER)\n",
    "\n",
    "    SCROLL_PAUSE_TIME = 0.5\n",
    "\n",
    "    # Get scroll height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Wait to load page\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return soup\n",
    "\n",
    "def coleta_dados_vagas(soup, nome_vaga):\n",
    "\n",
    "        lista_de_vagas = soup.find_all('div', {'class':'sc-a3bd7ea-0 HCzvP'})\n",
    "\n",
    "        list_columns = [\n",
    "                'site_da_vaga',\n",
    "                'link_site',\n",
    "                'link_origem',\n",
    "                'data_publicacao',\n",
    "                'data_expiracao',\n",
    "                'data_coleta',\n",
    "                'posicao',\n",
    "                'titulo_vaga',\n",
    "                'local',\n",
    "                'modalidade',\n",
    "                'nome_empresa',\n",
    "                'contrato',\n",
    "                'regime',\n",
    "                'pcd',\n",
    "                'beneficios',\n",
    "                'codigo_vaga',\n",
    "                'descricao'\n",
    "        ]\n",
    "\n",
    "        df_vagas = pd.DataFrame(\n",
    "                columns = list_columns\n",
    "            )\n",
    "\n",
    "        for vaga in lista_de_vagas:\n",
    "\n",
    "            df_aux = pd.DataFrame(\n",
    "                columns = list_columns\n",
    "            )\n",
    "\n",
    "            ## Buscando informações na própria página de pesquisa da Gupy\n",
    "\n",
    "            df_aux.loc[0,'site_da_vaga'] = 'Gupy'\n",
    "\n",
    "            df_aux.loc[0,'link_site'] = vaga.findAll('a')[0]['href']\n",
    "\n",
    "            df_aux.loc[0,'link_origem'] = vaga.findAll('a')[0]['href']\n",
    "\n",
    "            df_aux.loc[0,'data_publicacao'] = vaga.findAll('p', {'class':'sc-dPyBCJ kyoAxx sc-1db88588-0 inqtnx'})[0].text\n",
    "\n",
    "            df_aux.loc[0,'data_coleta'] = datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "            df_aux.loc[0,'posicao'] = nome_vaga\n",
    "\n",
    "            df_aux.loc[0,'titulo_vaga'] = vaga.findAll('h2')[0].text\n",
    "\n",
    "            df_aux.loc[0,'nome_empresa'] = vaga.findAll('p', {'class':'sc-dPyBCJ kyoAxx sc-a3bd7ea-6 cQyvth'})[0].text\n",
    "\n",
    "            try: \n",
    "                df_aux.loc[0,'local'] = vaga.findAll('span', {'class',\"sc-23336bc7-1 cezNaf\"})[0].text\n",
    "            except:\n",
    "                df_aux.loc[0,'local'] = np.nan\n",
    "\n",
    "            try:\n",
    "                df_aux.loc[0,'modalidade'] = vaga.findAll('span', {'class',\"sc-23336bc7-1 cezNaf\"})[1].text\n",
    "            except:\n",
    "                df_aux.loc[0,'modalidade'] = np.nan\n",
    "\n",
    "            try:\n",
    "                df_aux.loc[0,'contrato'] = vaga.findAll('span', {'class',\"sc-23336bc7-1 cezNaf\"})[2].text\n",
    "            except:\n",
    "                df_aux.loc[0,'contrato'] = np.nan\n",
    "\n",
    "            df_aux.loc[0, 'regime'] = np.nan\n",
    "\n",
    "            try:\n",
    "                df_aux.loc[0,'pcd'] = vaga.findAll('span', {'class',\"sc-23336bc7-1 cezNaf\"})[3].text\n",
    "            except:\n",
    "                df_aux.loc[0,'pcd'] = np.nan\n",
    "\n",
    "            df_aux.loc[0,'beneficios'] = np.nan\n",
    "\n",
    "            ## Request da página da vaga\n",
    "\n",
    "            try:\n",
    "                response = requests.get(vaga.findAll('a')[0]['href']) # link da vaga\n",
    "                page = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "                try:\n",
    "                    lista_descricao_em_texto = [page.findAll('div', {'data-testid':'text-section'})[i].text for i in range(len(page.findAll('div', {'data-testid':'text-section'})))]\n",
    "                    descricao_completa = '\\n'.join(lista_descricao_em_texto)\n",
    "                    df_aux.loc[0,'descricao'] = descricao_completa\n",
    "                except:\n",
    "                    df_aux.loc[0,'descricao'] = np.nan\n",
    "\n",
    "                try:\n",
    "                    df_aux.loc[0,'codigo_vaga'] = json.loads(page.findAll('script', {'id':'__NEXT_DATA__'})[0].get_text())['props']['pageProps']['job']['id']\n",
    "                except:\n",
    "                    df_aux.loc[0,'codigo_vaga'] = np.nan\n",
    "\n",
    "                try:\n",
    "                    df_aux.loc[0,'data_expiracao'] = json.loads(page.findAll('script', {'id':'__NEXT_DATA__'})[0].get_text())['props']['pageProps']['job']['expiresAt']\n",
    "                except:\n",
    "                    df_aux.loc[0,'data_expiracao'] = np.nan\n",
    "            except:\n",
    "                df_aux.loc[0,'descricao'] = np.nan\n",
    "                df_aux.loc[0,'codigo_vaga'] = np.nan\n",
    "                df_aux.loc[0,'data_expiracao'] = np.nan\n",
    "            \n",
    "            df_vagas = pd.concat([df_vagas, df_aux], axis=0, ignore_index=True)\n",
    "            \n",
    "        df_vagas = df_vagas.reset_index(drop=True)\n",
    "\n",
    "        nome_vaga = nome_vaga.replace(' ', '_').lower()\n",
    "        \n",
    "        df_vagas.to_excel(f'../data/data_raw/tmp/data_csv/{nome_vaga}_gupy.xlsx', index=False)\n",
    "\n",
    "        return df_vagas\n",
    "\n",
    "def busca_vagas(nome_vaga):\n",
    "\n",
    "    LINK = 'https://portal.gupy.io/'\n",
    "\n",
    "    vagas_gupy_page = vitrine_vagas_gupy(LINK,nome_vaga)\n",
    "\n",
    "    df_vagas_gupy = coleta_dados_vagas(vagas_gupy_page, nome_vaga)\n",
    "\n",
    "    return df_vagas_gupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_posicoes = ['Analista de Dados', 'Cientista de Dados', 'Engenheiro de Dados']\n",
    "df_vagas_full = pd.DataFrame()\n",
    "\n",
    "for posicao in lista_posicoes:\n",
    "    \n",
    "    df_vagas_aux = busca_vagas(posicao)\n",
    "\n",
    "    df_vagas_full = pd.concat([df_vagas_full, df_vagas_aux], axis = 0)\n",
    "    \n",
    "df_vagas_full.reset_index()\n",
    "\n",
    "df_vagas_full.to_excel('../data/data_raw/vagas_gupy_raw.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "empresa_jr",
   "language": "python",
   "name": "empresa_jr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
